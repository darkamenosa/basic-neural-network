# basic-neural-network
Basic neural network implementation.

Including:

* Activation: 
1. ReLU (forward and backward)
2. Sigmoid (forward and backward) 

* Regularization:
1. L2 regularization
2. Dropout
